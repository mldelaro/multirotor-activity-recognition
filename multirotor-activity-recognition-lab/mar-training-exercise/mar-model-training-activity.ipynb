{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Multirotor Activity Recognition - TRAINING YOUR MODEL\n",
    "\n",
    "Welcome to this training module that teaches you how to perform activity recognition with the help of a hosted Machine Learning instance. This notebook will guide you through the process of extracting information from a large dataset of existing IMU flight data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1) Getting started\n",
    "\n",
    "Let's start by importing libraries and defining runtime constants we will be running during the course of this training module. These constants will help keep track of file directories we will be interacting with."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[STEP 1 - START]===\n",
      "\n",
      "Setting up activity runtime environment...\n",
      "Importing used libraries...\n",
      "Defining runtime constants...\n",
      "Defining custom functions...\n",
      "[DONE] Runtime initialized\n",
      "\n",
      "===[STEP 1 - END]===\n"
     ]
    }
   ],
   "source": [
    "print('===[STEP 1 - START]===\\n')\n",
    "print('Setting up activity runtime environment...')\n",
    "\n",
    "print('Importing used libraries...')\n",
    "from IPython.display import Markdown, display\n",
    "import json\n",
    "import boto3\n",
    "import boto3.session\n",
    "import string\n",
    "import os\n",
    "import csv\n",
    "import numpy as np\n",
    "\n",
    "print('Defining runtime constants...')\n",
    "s3_workspace_bucket = 'mldelarosa-thesis'\n",
    "s3_subdir_group_training_flight_log = 'mar-lab-workspace/exercise-training/group-training-dataset/'\n",
    "s3_subdir_group_training_dataset = 'mar-lab-workspace/exercise-training/group-training-dataset/'\n",
    "\n",
    "jupyter_subdir_group_training_dataset = './data/training-group-dataset/'\n",
    "jupyter_subdir_group_training_flight_log = './data/training-group-logs/'\n",
    "jupyter_subdir_group_workspace = './data/group-workspace/'\n",
    "\n",
    "print('Defining custom functions...')\n",
    "def make_path(file_path):\n",
    "    directory = os.path.dirname(file_path)\n",
    "    if not os.path.exists(directory):\n",
    "        os.makedirs(directory)\n",
    "\n",
    "def get_s3_client():\n",
    "    session = boto3.session.Session()\n",
    "    s3 = session.resource(service_name='s3', verify=True)\n",
    "    return s3.meta.client\n",
    "\n",
    "\n",
    "def print_markup(string):\n",
    "    display(Markdown(string))\n",
    "    \n",
    "def print_file_preview(file, lines, hasHeader):\n",
    "    stringFormat = ''\n",
    "    print('\\n\\n===== FILE PREVIEW: ' + file + ' =====')\n",
    "    if hasHeader:\n",
    "        with open(file) as fileToPreview:\n",
    "            try:\n",
    "                fileIter = [next(fileToPreview) for x in range(lines)]\n",
    "                for x in range(lines): \n",
    "                    lineCSV = fileIter[x].rstrip().split(',')\n",
    "                    if x == 0:\n",
    "                        numOfColumns = len(lineCSV)\n",
    "                        dash = '-' * (17 * numOfColumns)\n",
    "                        stringFormat = '{:15.15}| ' * numOfColumns\n",
    "                        print(dash)\n",
    "                        print(stringFormat.format(*lineCSV))\n",
    "                        print(dash)\n",
    "                    else:\n",
    "                        print(stringFormat.format(*lineCSV))\n",
    "            except StopIteration:\n",
    "                print(\"*File too short for preview*\")\n",
    "                pass\n",
    "                    \n",
    "    else:\n",
    "        with open(file) as fileToPreview:\n",
    "            fileIter = [next(fileToPreview) for x in range(lines)]\n",
    "            for x in range(lines):\n",
    "                lineCSV = fileIter[x].rstrip().split(',')\n",
    "                if x == 0:\n",
    "                    numOfColumns = len(lineCSV)\n",
    "                    stringFormat = '{:16.16}| ' * numOfColumns\n",
    "                print(stringFormat.format(*lineCSV)[0:125])\n",
    "    \n",
    "    print('...')\n",
    "    print('===== END FILE PREVIEW =====\\n\\n')\n",
    "\n",
    "GROUP_NAME='held-with-manual-control'\n",
    "\n",
    "print('[DONE] Runtime initialized')\n",
    "print('\\n===[STEP 1 - END]===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 2) Download MAR Dataset for training the model\n",
    "To start training our linear classifier, let's download the data necessary for training our model. The __imu-database-training-set__ file downloaded in this step is a CSV data file that holds a large number of IMU entries recorded from reliable flight sessions. Each row is labelled so that the linear classifier knows what class of data an IMU entry belongs to."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[STEP 2 - DOWNLOAD TRAINING DATA SET]===\n",
      "\n",
      "Please enter the name of your lab group's training dataset:\n",
      "Group Name: poorly-trained\n",
      "Downloading prepped training data...\n",
      "Downloading from: mar-lab-workspace/exercise-training/group-training-dataset/poorly-trained/imu-data-log-latest\n",
      "Downloading to: ./data/exercise-training-session/poorly-trained/imu-db/imu-database-training-set.csv\n",
      "Downloading prepped evaluation data...\n",
      "Downloading from: mar-lab-workspace/exercise-training/group-training-dataset/poorly-trained/imu-data-log-latest\n",
      "Downloading to: ./data/exercise-training-session/poorly-trained/imu-db/imu-database-evaluation-set.csv\n",
      "Download completed...\n",
      "\n",
      "\n",
      "===== FILE PREVIEW: ./data/exercise-training-session/poorly-trained/imu-db/imu-database-training-set.csv =====\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "label          | accelerometer_x| accelerometer_y| accelerometer_z| gyrometer_x    | gyrometer_y    | gyrometer_z    | \n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "false-flight   | -0.0134277     | 0.017334       | -1.04468       | 0.0356092      | 0.0268245      | -0.0084336     | \n",
      "neutral        | -0.0141602     | 0.0146484      | -1.03906       | -0.0101923     | -0.00370992    | -0.0618687     | \n",
      "neutral        | -0.0148926     | 0.017334       | -1.0415        | -0.0178259     | -0.0342442     | -0.0160672     | \n",
      "neutral        | -0.0141602     | 0.0144043      | -1.04297       | -0.0559939     | 0.0115572      | 0.0144672      | \n",
      "neutral        | -0.00952148    | 0.0114746      | -1.03882       | -0.0178259     | -0.0647787     | -0.038968      | \n",
      "neutral        | -0.0117188     | 0.0151367      | -1.04102       | 0.0279756      | -0.0342442     | 0.0297344      | \n",
      "neutral        | -0.0163574     | 0.0153809      | -1.04004       | -0.0407267     | 0.0115572      | -0.0084336     | \n",
      "neutral        | -0.015625      | 0.015625       | -1.04346       | -0.0788947     | -0.0113436     | 0.0221008      | \n",
      "neutral        | -0.0117188     | 0.0185547      | -1.04102       | -0.0254596     | 0.0268245      | 0.0144672      | \n",
      "...\n",
      "===== END FILE PREVIEW =====\n",
      "\n",
      "\n",
      "\n",
      "\n",
      "===== FILE PREVIEW: ./data/exercise-training-session/poorly-trained/imu-db/imu-database-evaluation-set.csv =====\n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "label          | accelerometer_x| accelerometer_y| accelerometer_z| gyrometer_x    | gyrometer_y    | gyrometer_z    | \n",
      "-----------------------------------------------------------------------------------------------------------------------\n",
      "false-flight   | -0.0134277     | 0.017334       | -1.04468       | 0.0356092      | 0.0268245      | -0.0084336     | \n",
      "neutral        | -0.0141602     | 0.0146484      | -1.03906       | -0.0101923     | -0.00370992    | -0.0618687     | \n",
      "neutral        | -0.0148926     | 0.017334       | -1.0415        | -0.0178259     | -0.0342442     | -0.0160672     | \n",
      "neutral        | -0.0141602     | 0.0144043      | -1.04297       | -0.0559939     | 0.0115572      | 0.0144672      | \n",
      "neutral        | -0.00952148    | 0.0114746      | -1.03882       | -0.0178259     | -0.0647787     | -0.038968      | \n",
      "neutral        | -0.0117188     | 0.0151367      | -1.04102       | 0.0279756      | -0.0342442     | 0.0297344      | \n",
      "neutral        | -0.0163574     | 0.0153809      | -1.04004       | -0.0407267     | 0.0115572      | -0.0084336     | \n",
      "neutral        | -0.015625      | 0.015625       | -1.04346       | -0.0788947     | -0.0113436     | 0.0221008      | \n",
      "neutral        | -0.0117188     | 0.0185547      | -1.04102       | -0.0254596     | 0.0268245      | 0.0144672      | \n",
      "...\n",
      "===== END FILE PREVIEW =====\n",
      "\n",
      "\n",
      "\n",
      "===[STEP 2 - END]===\n"
     ]
    }
   ],
   "source": [
    "print('===[STEP 2 - DOWNLOAD TRAINING DATA SET]===\\n')\n",
    "\n",
    "print('Please enter the name of your lab group\\'s training dataset:')\n",
    "GROUP_NAME = (input('Group Name: ') or \"held-with-manual-control\")\n",
    "\n",
    "\n",
    "# Download prepped training data\n",
    "s3_filepath_mar_training_database = 'mar-lab-workspace/exercise-training/group-training-dataset/' + GROUP_NAME + '/imu-data-log-latest'\n",
    "jupyter_filepath_mar_training_database = './data/exercise-training-session/' + GROUP_NAME + '/imu-db/imu-database-training-set.csv'\n",
    "\n",
    "# Download prepped evaluation data\n",
    "s3_filepath_mar_evaluation_database = 'mar-lab-workspace/exercise-training/group-training-dataset/' + GROUP_NAME + '/imu-data-log-latest'\n",
    "jupyter_filepath_mar_evaluation_database = './data/exercise-training-session/' + GROUP_NAME + '/imu-db/imu-database-evaluation-set.csv'\n",
    "\n",
    "client = get_s3_client()\n",
    "print('Downloading prepped training data...')\n",
    "print('Downloading from: ' + s3_filepath_mar_training_database)\n",
    "print('Downloading to: ' + jupyter_filepath_mar_training_database)\n",
    "make_path(jupyter_filepath_mar_training_database)\n",
    "group_flight_record = client.download_file(Bucket='mldelarosa-thesis',\n",
    "                                           Key=s3_filepath_mar_training_database,\n",
    "                                           Filename=jupyter_filepath_mar_training_database)\n",
    "\n",
    "print('Downloading prepped evaluation data...')\n",
    "print('Downloading from: ' + s3_filepath_mar_evaluation_database)\n",
    "print('Downloading to: ' + jupyter_filepath_mar_evaluation_database)\n",
    "make_path(jupyter_filepath_mar_evaluation_database)\n",
    "group_flight_record = client.download_file(Bucket='mldelarosa-thesis',\n",
    "                                           Key=s3_filepath_mar_evaluation_database,\n",
    "                                           Filename=jupyter_filepath_mar_evaluation_database)\n",
    "\n",
    "print('Download completed...')\n",
    "print_file_preview(jupyter_filepath_mar_training_database, 10, 1)\n",
    "print_file_preview(jupyter_filepath_mar_evaluation_database, 10, 1)\n",
    "print('\\n===[STEP 2 - END]===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3) Extract samples by label and store them in subdirectories\n",
    "Now that we have the data necessary to train our linear model, we organize the data by class in order to start the training process. For each unique label value in the __imu-database-training-set__ file, a new directory is created for that specific label. Rows are than compiled into multiple CSV files organized by label. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[STEP 3 - SEPARATING DATASETS BY LABEL]===\n",
      "\n",
      "Extracting labels for training dataset...\n",
      "Extracting data labels from: ./data/exercise-training-session/poorly-trained/imu-db/imu-database-training-set.csv\n",
      "Extracting data labels to directories in: [./data/exercise-training-session/poorly-trained/imu-db/]...\n",
      "\n",
      "\n",
      "===== FILE PREVIEW: ./data/exercise-training-session/poorly-trained/imu-db/down/down-sample-0.csv =====\n",
      "------------------------------------------------------------------------------------------------------\n",
      "accelerometer_x| accelerometer_y| accelerometer_z| gyrometer_x    | gyrometer_y    | gyrometer_z    | \n",
      "------------------------------------------------------------------------------------------------------\n",
      "-0.0117188     | 0.0168457      | -1.03809       | -0.00255877    | -0.00370992    | -0.000799999   | \n",
      "-0.0126953     | 0.0175781      | -1.04004       | 0.0356092      | 0.0420916      | -0.10767       | \n",
      "-0.0134277     | 0.0168457      | -1.04199       | 0.0127084      | 0.0497252      | -0.100037      | \n",
      "-0.0148926     | 0.0170898      | -1.04272       | 0.0585099      | 0.0649924      | -0.038968      | \n",
      "-0.012207      | 0.0170898      | -1.03931       | -0.0407267     | 0.0115572      | -0.0237008     | \n",
      "-0.0141602     | 0.0146484      | -1.03857       | -0.00255877    | 0.0497252      | -0.0542351     | \n",
      "-0.0124512     | 0.0148926      | -1.03711       | -0.0178259     | 0.0497252      | 0.0068336      | \n",
      "-0.0107422     | 0.0180664      | -1.03809       | -0.0407267     | 0.0802596      | -0.000799999   | \n",
      "-0.0126953     | 0.0151367      | -1.03906       | -0.0559939     | -0.00370992    | -0.0313344     | \n",
      "...\n",
      "===== END FILE PREVIEW =====\n",
      "\n",
      "\n",
      "Extracting labels for evaluation dataset...\n",
      "Extracting data labels from: ./data/exercise-training-session/poorly-trained/imu-db/imu-database-evaluation-set.csv\n",
      "Extracting data labels to directories in: [./data/exercise-training-session/poorly-trained/imu-db/]...\n",
      "\n",
      "\n",
      "===== FILE PREVIEW: ./data/exercise-training-session/poorly-trained/imu-db/down/down-sample-0.csv =====\n",
      "------------------------------------------------------------------------------------------------------\n",
      "accelerometer_x| accelerometer_y| accelerometer_z| gyrometer_x    | gyrometer_y    | gyrometer_z    | \n",
      "------------------------------------------------------------------------------------------------------\n",
      "-0.0117188     | 0.0168457      | -1.03809       | -0.00255877    | -0.00370992    | -0.000799999   | \n",
      "-0.0126953     | 0.0175781      | -1.04004       | 0.0356092      | 0.0420916      | -0.10767       | \n",
      "-0.0134277     | 0.0168457      | -1.04199       | 0.0127084      | 0.0497252      | -0.100037      | \n",
      "-0.0148926     | 0.0170898      | -1.04272       | 0.0585099      | 0.0649924      | -0.038968      | \n",
      "-0.012207      | 0.0170898      | -1.03931       | -0.0407267     | 0.0115572      | -0.0237008     | \n",
      "-0.0141602     | 0.0146484      | -1.03857       | -0.00255877    | 0.0497252      | -0.0542351     | \n",
      "-0.0124512     | 0.0148926      | -1.03711       | -0.0178259     | 0.0497252      | 0.0068336      | \n",
      "-0.0107422     | 0.0180664      | -1.03809       | -0.0407267     | 0.0802596      | -0.000799999   | \n",
      "-0.0126953     | 0.0151367      | -1.03906       | -0.0559939     | -0.00370992    | -0.0313344     | \n",
      "...\n",
      "===== END FILE PREVIEW =====\n",
      "\n",
      "\n",
      "Samples have been extracted by label\n",
      "\n",
      "===[STEP 3 - END]===\n"
     ]
    }
   ],
   "source": [
    "def extract_label_sets_from_file(data_filepath, destination_dir):\n",
    "    labelled_file = open(data_filepath, 'r')\n",
    "    \n",
    "    csv_columns = ['accelerometer_x','accelerometer_y','accelerometer_z','gyrometer_x','gyrometer_y','gyrometer_z']\n",
    "    \n",
    "    running_index = 0\n",
    "    running_label = ''\n",
    "    running_sample_index = {}\n",
    "    running_sample_filename = ''\n",
    "    labelled_row_reader = csv.DictReader(labelled_file)\n",
    "    print('Extracting data labels to directories in: [' + destination_dir + ']...')\n",
    "    for labelled_row in labelled_row_reader:\n",
    "        if(running_label != labelled_row['label']):\n",
    "            # Iterate sample file index for the current label\n",
    "            running_label = labelled_row['label']\n",
    "            if running_label in running_sample_index.keys():\n",
    "                running_sample_index[running_label] = running_sample_index[running_label] + 1;\n",
    "            else:\n",
    "                running_sample_index[running_label] = 0;\n",
    "            running_index = 0;\n",
    "            running_sample_filename = destination_dir + \\\n",
    "                                        running_label + '/' + \\\n",
    "                                        running_label + '-sample-' + \\\n",
    "                                        str(running_sample_index[running_label]) + '.csv'\n",
    "            make_path(running_sample_filename)\n",
    "            running_sample_file = open(running_sample_filename, 'w')\n",
    "            running_sample_file.write(','.join(csv_columns) + '\\n')\n",
    "            running_sample_file.write(labelled_row['accelerometer_x'] \\\n",
    "                                + ',' + labelled_row['accelerometer_y'] \\\n",
    "                                + ',' + labelled_row['accelerometer_z'] \\\n",
    "                                + ',' + labelled_row['gyrometer_x'] \\\n",
    "                                + ',' + labelled_row['gyrometer_y'] \\\n",
    "                                + ',' + labelled_row['gyrometer_z'] + '\\n')\n",
    "        else:\n",
    "            running_index = running_index + 1\n",
    "            running_sample_file.write(labelled_row['accelerometer_x'] \\\n",
    "                                + ',' + labelled_row['accelerometer_y'] \\\n",
    "                                + ',' + labelled_row['accelerometer_z'] \\\n",
    "                                + ',' + labelled_row['gyrometer_x'] \\\n",
    "                                + ',' + labelled_row['gyrometer_y'] \\\n",
    "                                + ',' + labelled_row['gyrometer_z'] + '\\n')\n",
    "    return running_sample_filename\n",
    "\n",
    "print('===[STEP 3 - SEPARATING DATASETS BY LABEL]===\\n')\n",
    "\n",
    "\n",
    "print('Extracting labels for training dataset...')\n",
    "s3_subdir_group_training_session = './data/exercise-training-session/' + GROUP_NAME + '/'\n",
    "print('Extracting data labels from: ' + jupyter_filepath_mar_training_database)\n",
    "lastSampleFile = extract_label_sets_from_file(jupyter_filepath_mar_training_database,\n",
    "                                                s3_subdir_group_training_session + 'imu-db/')\n",
    "print_file_preview(lastSampleFile, 10, 1)\n",
    "\n",
    "print('Extracting labels for evaluation dataset...')\n",
    "s3_subdir_group_evaluation_session = './data/exercise-training-session/' + GROUP_NAME + '/'\n",
    "print('Extracting data labels from: ' + jupyter_filepath_mar_evaluation_database)\n",
    "lastSampleFile = extract_label_sets_from_file(jupyter_filepath_mar_evaluation_database,\n",
    "                                              s3_subdir_group_evaluation_session + 'imu-db/')\n",
    "print_file_preview(lastSampleFile, 10, 1)\n",
    "\n",
    "print('Samples have been extracted by label')\n",
    "print('\\n===[STEP 3 - END]===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4) Calculate features from sliding windows on each label\n",
    "Features are calculated windows of data organized by label. Such features include average, median, and possibly variance. In this cell, the csv data is sliced into windows of data for which features can be calculated from."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[STEP 4 - EXTRACT FEATURES FOR EACH LABEL]===\n",
      "\n",
      "Calculating features for samples in the training data directory...\n",
      "\n",
      "Extracting features for the label [backward]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/backward/*.csv\n",
      "Extracting features for the label [forward]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/forward/*.csv\n",
      "Extracting features for the label [left]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/left/*.csv\n",
      "Extracting features for the label [neutral]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/neutral/*.csv\n",
      "Extracting features for the label [right]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/right/*.csv\n",
      "Extracting features for the label [up]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/up/*.csv\n",
      "Extracting features for the label [down]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/down/*.csv\n",
      "\n",
      "The features have been extracted and stored into: ./data/exercise-training-session/poorly-trained/training-feature-data-latest.csv\n",
      "\n",
      "\n",
      "===== FILE PREVIEW: ./data/exercise-training-session/poorly-trained/training-feature-data-latest.csv =====\n",
      "backward        | -0.0128784      | 0.01611327500000| -1.0408925      | 0.029883975     | 0.04018330000000| -0.016067175    |\n",
      "backward        | -0.012756325    | 0.01586915000000| -1.0417475      | -0.0006503749999| 0.0420917       | -0.02942595     |\n",
      "backward        | -0.012634275    | 0.01531982499999| -1.0413825      | -0.0121007499999| 0.044000075     | -0.048509925    |\n",
      "backward        | -0.0131836      | 0.01538087499999| -1.0411375      | -0.0063755749999| 0.01728244999999| -0.04087635     |\n",
      "backward        | -0.0127563500000| 0.01507570000000| -1.041625       | 0.00698322499999| 0.038274825     | -0.03705955     |\n",
      "...\n",
      "===== END FILE PREVIEW =====\n",
      "\n",
      "\n",
      "Calculating features for samples in the evaluation data directory...\n",
      "\n",
      "Extracting features for the label [backward]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/backward/*.csv\n",
      "\n",
      "Extracting features for the label [forward]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/forward/*.csv\n",
      "\n",
      "Extracting features for the label [left]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/left/*.csv\n",
      "\n",
      "Extracting features for the label [neutral]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/neutral/*.csv\n",
      "\n",
      "Extracting features for the label [right]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/right/*.csv\n",
      "\n",
      "Extracting features for the label [up]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/up/*.csv\n",
      "\n",
      "Extracting features for the label [down]...\n",
      "Extracting features into the filepath ./data/exercise-training-session/poorly-trained/imu-db/down/*.csv\n",
      "\n",
      "\n",
      "The features have been extracted and stored into: ./data/exercise-training-session/poorly-trained/evaluation-feature-data-latest.csv\n",
      "\n",
      "\n",
      "===== FILE PREVIEW: ./data/exercise-training-session/poorly-trained/evaluation-feature-data-latest.csv =====\n",
      "backward        | -0.0128784      | 0.01611327500000| -1.0408925      | 0.029883975     | 0.04018330000000| -0.016067175    |\n",
      "backward        | -0.012756325    | 0.01586915000000| -1.0417475      | -0.0006503749999| 0.0420917       | -0.02942595     |\n",
      "backward        | -0.012634275    | 0.01531982499999| -1.0413825      | -0.0121007499999| 0.044000075     | -0.048509925    |\n",
      "backward        | -0.0131836      | 0.01538087499999| -1.0411375      | -0.0063755749999| 0.01728244999999| -0.04087635     |\n",
      "backward        | -0.0127563500000| 0.01507570000000| -1.041625       | 0.00698322499999| 0.038274825     | -0.03705955     |\n",
      "...\n",
      "===== END FILE PREVIEW =====\n",
      "\n",
      "\n",
      "\n",
      "===[STEP 4 - END]===\n"
     ]
    }
   ],
   "source": [
    "print('===[STEP 4 - EXTRACT FEATURES FOR EACH LABEL]===\\n')\n",
    "\n",
    "from itertools import islice\n",
    "\n",
    "feature_csv_columns = ['average', 'median']\n",
    "imu_data_columns = ['accelerometer_x','accelerometer_y','accelerometer_z','gyrometer_x','gyrometer_y','gyrometer_z']\n",
    "imu_label_vocabulary = ['backward', 'forward', 'left', 'neutral', 'right', 'up', 'down']\n",
    "\n",
    "def feature_average(data_sample):\n",
    "    fSum = 0;\n",
    "    nIndex = 0;\n",
    "    for data in data_sample:\n",
    "        fSum = fSum + data\n",
    "        nIndex = nIndex + 1\n",
    "    return float(fSum / nIndex)\n",
    "\n",
    "def feature_variance(data_sample):\n",
    "    return np.var(data_sample)\n",
    "\n",
    "def feature_median(data_sample):\n",
    "    return np.median(data_sample, axis=0)\n",
    "\n",
    "feature_calculations = {\n",
    "    'average' : feature_average,\n",
    "    'median' : feature_median\n",
    "}\n",
    "\n",
    "# Read a *.csv file and extract the sliding window\n",
    "import collections\n",
    "def extract_features_from_imu_data_samples_for_label(data_sample_filepath, features_filepath, data_label):\n",
    "#     print('Extracting for feature: ', data_label, 'from', data_sample_filepath)\n",
    "    with open(data_sample_filepath, 'r') as csv_file:\n",
    "        # extract data records by row\n",
    "        reader = csv.DictReader(csv_file)\n",
    "        sliding_windows = []\n",
    "        sliding_index = 0\n",
    "        window_step_forward = 1\n",
    "        window_length = 4\n",
    "        \n",
    "        # extract sliding windows from rows\n",
    "        sliding_window_csv = []\n",
    "        for row in reader:\n",
    "            sliding_window_csv.append(row)\n",
    "            if(len(sliding_window_csv) == window_length + 1):\n",
    "                del sliding_window_csv[0]\n",
    "            if(sliding_index % window_step_forward == 0 and len(sliding_window_csv) == window_length):\n",
    "                sliding_windows.append(list(sliding_window_csv))\n",
    "            sliding_index = sliding_index + 1\n",
    "        running_window_lines = []\n",
    "                \n",
    "        for feature_name, feature_func in feature_calculations.items():\n",
    "            for imu_data_column in imu_data_columns:\n",
    "                window_sequences = []\n",
    "                for sliding_window in sliding_windows:\n",
    "                    window_sequence = []\n",
    "                    for window in sliding_window:\n",
    "                        window_sequence.append(float(window[imu_data_column]))\n",
    "                    window_sequences.append(window_sequence)\n",
    "#                 print(imu_data_column, ' - ', window_sequences)\n",
    "                \n",
    "                window_index = 0\n",
    "                comma_index = 0;\n",
    "                window_count = len(window_sequences)\n",
    "                while len(running_window_lines) < window_count:\n",
    "                    running_window_lines.append('')\n",
    "                for window in window_sequences:\n",
    "                    running_window_lines[window_index % window_count] += (str(feature_func(window))) + ','\n",
    "#                   running_window_lines[window_index % window_count] += ('{' + feature_name + ':' + imu_data_column + ':' + str(feature_func(window))) + '},'\n",
    "#                   print(imu_data_column, ' _ ', feature_name, feature_func(window))\n",
    "                    window_index = window_index + 1\n",
    "#     print('PRINT FOR WINDOW ', window_count , running_window_lines)\n",
    "\n",
    "    with open(features_filepath, 'a') as features_file:\n",
    "        for feature_line in running_window_lines:\n",
    "            features_file.write(data_label + ',' + feature_line[:-1] + '\\n')\n",
    "            \n",
    "\n",
    "# Define labels to iterate through the raw IMU data directories\n",
    "import glob\n",
    "import os\n",
    "labels = set()\n",
    "labels = ['backward', 'forward', 'left', 'neutral', 'right', 'up', 'down']\n",
    "\n",
    "# clear out training data from the last execution\n",
    "with open(s3_subdir_group_training_session + 'training-feature-data-latest.csv', 'w') as file:\n",
    "    file.write('')\n",
    "    \n",
    "\n",
    "# Iterate through each raw IMU data sample and extract their features:\n",
    "print('Calculating features for samples in the training data directory...\\n')\n",
    "for labelled_features in labels:\n",
    "    print('Extracting features for the label [' + labelled_features + ']...')\n",
    "    print('Extracting features into the filepath ' + s3_subdir_group_training_session + 'imu-db/' + labelled_features + '/*.csv')\n",
    "    for raw_data_dir in glob.glob(s3_subdir_group_training_session + 'imu-db/' + labelled_features + '/*.csv', recursive=False):\n",
    "        extract_features_from_imu_data_samples_for_label(raw_data_dir, s3_subdir_group_training_session + 'training-feature-data-latest.csv', labelled_features)\n",
    "print('\\nThe features have been extracted and stored into: ' + s3_subdir_group_training_session + 'training-feature-data-latest.csv')\n",
    "print_file_preview(s3_subdir_group_training_session + 'training-feature-data-latest.csv', 5, 0)\n",
    "\n",
    "\n",
    "print('Calculating features for samples in the evaluation data directory...\\n')\n",
    "for labelled_features in labels:\n",
    "    print('Extracting features for the label [' + labelled_features + ']...')\n",
    "    print('Extracting features into the filepath ' + s3_subdir_group_evaluation_session + 'imu-db/' + labelled_features + '/*.csv')\n",
    "    for raw_data_dir in glob.glob(s3_subdir_group_evaluation_session + 'imu-db/' + labelled_features + '/*.csv', recursive=False):\n",
    "        extract_features_from_imu_data_samples_for_label(raw_data_dir, s3_subdir_group_evaluation_session + 'evaluation-feature-data-latest.csv', labelled_features)\n",
    "    print('')\n",
    "print('\\nThe features have been extracted and stored into: ' + s3_subdir_group_evaluation_session + 'evaluation-feature-data-latest.csv')\n",
    "print_file_preview(s3_subdir_group_evaluation_session + 'evaluation-feature-data-latest.csv', 5, 0)\n",
    "\n",
    "print('\\n===[STEP 4 - END]===')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "## Step 5) Train a linear classifier\n",
    "A linear classifier is trained upon the extracted feature set calculated from the previous step."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "===[STEP 5 - TRAIN A LINEAR CLASSIFIER]===\n",
      "Training at epoch 0 ...\n",
      "Training at epoch 1 ...\n",
      "Training at epoch 2 ...\n",
      "Training at epoch 3 ...\n",
      "Training at epoch 4 ...\n",
      "Training at epoch 5 ...\n",
      "Training at epoch 6 ...\n",
      "Training at epoch 7 ...\n",
      "Training at epoch 8 ...\n",
      "Training at epoch 9 ...\n",
      "Training at epoch 10 ...\n",
      "Training at epoch 11 ...\n",
      "Training at epoch 12 ...\n",
      "Training at epoch 13 ...\n",
      "Training at epoch 14 ...\n",
      "Training at epoch 15 ...\n",
      "Training at epoch 16 ...\n",
      "Training at epoch 17 ...\n",
      "Training at epoch 18 ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Detail execution of epoch 19\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-779\n",
      "INFO:tensorflow:Saving checkpoints for 780 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 519.5571, step = 780\n",
      "INFO:tensorflow:Saving checkpoints for 820 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 59.425507.\n",
      "Evaluating model...\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-05-18:47:48\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-820\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-05-18:47:48\n",
      "INFO:tensorflow:Saving dict for global step 820: accuracy = 0.45036742, average_loss = 1.6704664, global_step = 820, loss = 327.12622\n",
      "{'accuracy': 0.45036742, 'average_loss': 1.6704664, 'loss': 327.12622, 'global_step': 820}\n",
      "================================================================================\n",
      "Training at epoch 20 ...\n",
      "Training at epoch 21 ...\n",
      "Training at epoch 22 ...\n",
      "Training at epoch 23 ...\n",
      "Training at epoch 24 ...\n",
      "Training at epoch 25 ...\n",
      "Training at epoch 26 ...\n",
      "Training at epoch 27 ...\n",
      "Training at epoch 28 ...\n",
      "Training at epoch 29 ...\n",
      "Training at epoch 30 ...\n",
      "Training at epoch 31 ...\n",
      "Training at epoch 32 ...\n",
      "Training at epoch 33 ...\n",
      "Training at epoch 34 ...\n",
      "Training at epoch 35 ...\n",
      "Training at epoch 36 ...\n",
      "Training at epoch 37 ...\n",
      "Training at epoch 38 ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Detail execution of epoch 39\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-1599\n",
      "INFO:tensorflow:Saving checkpoints for 1600 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 510.57092, step = 1600\n",
      "INFO:tensorflow:Saving checkpoints for 1640 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 62.134357.\n",
      "Evaluating model...\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-05-18:48:27\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-1640\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-05-18:48:28\n",
      "INFO:tensorflow:Saving dict for global step 1640: accuracy = 0.45036742, average_loss = 1.6667002, global_step = 1640, loss = 326.3887\n",
      "{'accuracy': 0.45036742, 'average_loss': 1.6667002, 'loss': 326.3887, 'global_step': 1640}\n",
      "================================================================================\n",
      "Training at epoch 40 ...\n",
      "Training at epoch 41 ...\n",
      "Training at epoch 42 ...\n",
      "Training at epoch 43 ...\n",
      "Training at epoch 44 ...\n",
      "Training at epoch 45 ...\n",
      "Training at epoch 46 ...\n",
      "Training at epoch 47 ...\n",
      "Training at epoch 48 ...\n",
      "Training at epoch 49 ...\n",
      "Training at epoch 50 ...\n",
      "Training at epoch 51 ...\n",
      "Training at epoch 52 ...\n",
      "Training at epoch 53 ...\n",
      "Training at epoch 54 ...\n",
      "Training at epoch 55 ...\n",
      "Training at epoch 56 ...\n",
      "Training at epoch 57 ...\n",
      "Training at epoch 58 ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Detail execution of epoch 59\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-2419\n",
      "INFO:tensorflow:Saving checkpoints for 2420 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 507.4826, step = 2420\n",
      "INFO:tensorflow:Saving checkpoints for 2460 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 61.20252.\n",
      "Evaluating model...\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-05-18:49:07\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-2460\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-05-18:49:08\n",
      "INFO:tensorflow:Saving dict for global step 2460: accuracy = 0.45036742, average_loss = 1.6649954, global_step = 2460, loss = 326.05484\n",
      "{'accuracy': 0.45036742, 'average_loss': 1.6649954, 'loss': 326.05484, 'global_step': 2460}\n",
      "================================================================================\n",
      "Training at epoch 60 ...\n",
      "Training at epoch 61 ...\n",
      "Training at epoch 62 ...\n",
      "Training at epoch 63 ...\n",
      "Training at epoch 64 ...\n",
      "Training at epoch 65 ...\n",
      "Training at epoch 66 ...\n",
      "Training at epoch 67 ...\n",
      "Training at epoch 68 ...\n",
      "Training at epoch 69 ...\n",
      "Training at epoch 70 ...\n",
      "Training at epoch 71 ...\n",
      "Training at epoch 72 ...\n",
      "Training at epoch 73 ...\n",
      "Training at epoch 74 ...\n",
      "Training at epoch 75 ...\n",
      "Training at epoch 76 ...\n",
      "Training at epoch 77 ...\n",
      "Training at epoch 78 ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Detail execution of epoch 79\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-3239\n",
      "INFO:tensorflow:Saving checkpoints for 3240 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 504.347, step = 3240\n",
      "INFO:tensorflow:Saving checkpoints for 3280 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 61.830063.\n",
      "Evaluating model...\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-05-18:49:46\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-3280\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-05-18:49:47\n",
      "INFO:tensorflow:Saving dict for global step 3280: accuracy = 0.45036742, average_loss = 1.6639104, global_step = 3280, loss = 325.84235\n",
      "{'accuracy': 0.45036742, 'average_loss': 1.6639104, 'loss': 325.84235, 'global_step': 3280}\n",
      "================================================================================\n",
      "Training at epoch 80 ...\n",
      "Training at epoch 81 ...\n",
      "Training at epoch 82 ...\n",
      "Training at epoch 83 ...\n",
      "Training at epoch 84 ...\n",
      "Training at epoch 85 ...\n",
      "Training at epoch 86 ...\n",
      "Training at epoch 87 ...\n",
      "Training at epoch 88 ...\n",
      "Training at epoch 89 ...\n",
      "Training at epoch 90 ...\n",
      "Training at epoch 91 ...\n",
      "Training at epoch 92 ...\n",
      "Training at epoch 93 ...\n",
      "Training at epoch 94 ...\n",
      "Training at epoch 95 ...\n",
      "Training at epoch 96 ...\n",
      "Training at epoch 97 ...\n",
      "Training at epoch 98 ...\n",
      "\n",
      "\n",
      "================================================================================\n",
      "Detail execution of epoch 99\n",
      "INFO:tensorflow:Create CheckpointSaverHook.\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-4059\n",
      "INFO:tensorflow:Saving checkpoints for 4060 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:loss = 503.67282, step = 4060\n",
      "INFO:tensorflow:Saving checkpoints for 4100 into ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt.\n",
      "INFO:tensorflow:Loss for final step: 61.964447.\n",
      "Evaluating model...\n",
      "INFO:tensorflow:Starting evaluation at 2018-11-05-18:50:26\n",
      "INFO:tensorflow:Restoring parameters from ../mar-classification-exercise/tmp/model/poorly-trained/model.ckpt-4100\n",
      "INFO:tensorflow:Finished evaluation at 2018-11-05-18:50:27\n",
      "INFO:tensorflow:Saving dict for global step 4100: accuracy = 0.45036742, average_loss = 1.6631198, global_step = 4100, loss = 325.68753\n",
      "{'accuracy': 0.45036742, 'average_loss': 1.6631198, 'loss': 325.68753, 'global_step': 4100}\n",
      "================================================================================\n",
      "\n",
      "================================================================================\n",
      "Final Model Evaluation\n",
      "  accuracy: 0.45036742091178894\n",
      "  average_loss: 1.6631197929382324\n",
      "  loss: 325.6875305175781\n",
      "  global_step: 4100\n",
      "================================================================================\n",
      "[DONE]\n",
      "\n",
      "===[STEP 5 - END]===\n"
     ]
    }
   ],
   "source": [
    "print('===[STEP 5 - TRAIN A LINEAR CLASSIFIER]===')\n",
    "\n",
    "import tensorflow as tf\n",
    "import os\n",
    "import shutil\n",
    "import sys\n",
    "\n",
    "# THIS MODEL-DIR POINTS TO THE CLASSIFICATION EXERCISE WORKSPACE\n",
    "model_dir = '../mar-classification-exercise/tmp/model/' + GROUP_NAME + '/'\n",
    "train_data = './data/exercise-training-session/' + GROUP_NAME + '/training-feature-data-latest.csv'\n",
    "#eval_data = './data/exercise-training-session/' + GROUP_NAME + '/training-feature-data-latest.csv\n",
    "eval_data = './data/exercise-training-session/' + GROUP_NAME + '/evaluation-feature-data-latest.csv'\n",
    "\n",
    "# delete the model directory\n",
    "shutil.rmtree(model_dir, ignore_errors=True)\n",
    "\n",
    "# declare feature columns within csv\n",
    "mean_acc_x = tf.feature_column.numeric_column(key='mean_acc_x', dtype=tf.float64);\n",
    "mean_acc_y = tf.feature_column.numeric_column(key='mean_acc_y', dtype=tf.float64);\n",
    "mean_acc_z = tf.feature_column.numeric_column(key='mean_acc_z', dtype=tf.float64);\n",
    "\n",
    "mean_gyro_roll = tf.feature_column.numeric_column(key='mean_gyro_roll', dtype=tf.float64);\n",
    "mean_gyro_pitch = tf.feature_column.numeric_column(key='mean_gyro_pitch', dtype=tf.float64);\n",
    "mean_gyro_yaw = tf.feature_column.numeric_column(key='mean_gyro_yaw', dtype=tf.float64);\n",
    "\n",
    "median_acc_x = tf.feature_column.numeric_column(key='median_acc_x', dtype=tf.float64);\n",
    "median_acc_y = tf.feature_column.numeric_column(key='median_acc_y', dtype=tf.float64);\n",
    "median_acc_z = tf.feature_column.numeric_column(key='median_acc_z', dtype=tf.float64);\n",
    "\n",
    "median_gyro_roll = tf.feature_column.numeric_column(key='median_gyro_roll', dtype=tf.float64);\n",
    "median_gyro_pitch = tf.feature_column.numeric_column(key='median_gyro_pitch', dtype=tf.float64);\n",
    "median_gyro_yaw = tf.feature_column.numeric_column(key='median_gyro_yaw', dtype=tf.float64);\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "# stack feature columns into a single array\n",
    "imu_window_feature_columns = [\n",
    "        mean_acc_x, mean_acc_y, mean_acc_z,\n",
    "        mean_gyro_roll, mean_gyro_pitch, mean_gyro_yaw,\n",
    "        median_acc_x, median_acc_y, median_acc_z,\n",
    "        median_gyro_roll, median_gyro_pitch, median_gyro_yaw,\n",
    "    ]\n",
    "\n",
    "run_config=tf.estimator.RunConfig().replace(\n",
    "    session_config=tf.ConfigProto(device_count={'GPU': 0})\n",
    ")\n",
    "\n",
    "def input_fn(data_file):\n",
    "    assert tf.gfile.Exists(data_file),('%s not found')\n",
    "    records_default = [['neutral'],\n",
    "                       [0.0], [0.0], [0.0],\n",
    "                       [0.0], [0.0], [0.0],\n",
    "                       [0.0], [0.0], [0.0],\n",
    "                       [0.0], [0.0], [0.0]]\n",
    "    csv_columns = [\n",
    "                    'label',\n",
    "                    'mean_acc_x','mean_acc_y','mean_acc_z',\n",
    "                    'mean_gyro_roll','mean_gyro_pitch','mean_gyro_yaw',\n",
    "                    'median_acc_x','median_acc_y','median_acc_z',\n",
    "                    'median_gyro_roll','median_gyro_pitch','median_gyro_yaw'\n",
    "    ]\n",
    "    \n",
    "    def parse_csv(value):\n",
    "        columns = tf.decode_csv(value, records_default)\n",
    "        features = dict(zip(csv_columns, columns))\n",
    "        labels = features.pop('label')\n",
    "        return features, labels\n",
    "    \n",
    "    dataset = tf.data.TextLineDataset(data_file)\n",
    "    dataset = dataset.shuffle(200)\n",
    "    dataset = dataset.map(parse_csv, 4)\n",
    "    dataset = dataset.batch(200)\n",
    "    return dataset\n",
    "\n",
    "model = tf.estimator.LinearClassifier(\n",
    "    model_dir=model_dir,\n",
    "    feature_columns=imu_window_feature_columns,\n",
    "    config=run_config,\n",
    "    n_classes=7,\n",
    "    label_vocabulary=['backward', 'forward', 'left', 'neutral', 'right', 'up', 'down']\n",
    ")\n",
    "\n",
    "# A Deep Neural Network Classifier can also be substituted for linear classifier\n",
    "# model = tf.estimator.DNNClassifier(\n",
    "#     model_dir=model_dir,\n",
    "#     feature_columns=imu_window_feature_columns,\n",
    "#     config=run_config,\n",
    "#     hidden_units=[100, 75, 50, 25],\n",
    "#     n_classes=7,\n",
    "#     label_vocabulary=['backward', 'forward', 'left', 'neutral', 'right', 'up', 'down']\n",
    "# )\n",
    "\n",
    "\n",
    "# Train and evaluate the model every certain number of epochs.\n",
    "# More information available at https://docs.aws.amazon.com/sagemaker/latest/dg/tf-training-inference-code-template.html\n",
    "for n in range(100):\n",
    "    # Display INFO logs from tensorflow every 10 iterations\n",
    "    if ((n+1) % 20 == 0):\n",
    "        print('\\n')\n",
    "        print('=' * 80)\n",
    "        print('Detail execution of epoch', n)\n",
    "        tf.logging.set_verbosity(tf.logging.INFO)\n",
    "    else:\n",
    "        print('Training at epoch', n, '...')\n",
    "        tf.logging.set_verbosity(tf.logging.ERROR)\n",
    "    \n",
    "    model.train(input_fn=lambda: input_fn(\n",
    "        train_data))\n",
    "    \n",
    "#     Detailed evaluation log of model's loss and accuracy during an epoch\n",
    "#     results = model.evaluate(input_fn=lambda: input_fn(\n",
    "#          eval_data))\n",
    "#     print(\"{},{},{}\".format(n, results['loss'], results['accuracy']))\n",
    "    \n",
    "    # Display evaluation metrics every 10 iterations\n",
    "    if ((n+1) % 20 == 0):\n",
    "        print (\"Evaluating model...\")\n",
    "        results = model.evaluate(input_fn=lambda: input_fn(\n",
    "        eval_data))\n",
    "        print(results)\n",
    "        print('=' * 80)\n",
    "\n",
    "print('')\n",
    "print('=' * 80)\n",
    "print (\"Final Model Evaluation\")\n",
    "\n",
    "for key in results:\n",
    "    print(\"  {}: {}\".format(key, results[key]))\n",
    "print('=' * 80)\n",
    "\n",
    "print('[DONE]')\n",
    "print('\\n===[STEP 5 - END]===')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tf.train.list_variables(model_dir)\n",
    "\n",
    "weightNames = model.get_variable_names()\n",
    "#weightValues = [model.get_variable_value(name) for name in wt_names]\n",
    "\n",
    "for name in weightNames:\n",
    "    print(name, ':\\n', model.get_variable_value(name), '\\n')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
